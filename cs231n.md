*Day1 22/10/11*

## Classify_image

## 数据驱动 Data_driven
1. Meaning: 堆数据，用数据自己去发现不同类物体在图像表现上的规则
2. Dataset：CIFAR10等
    
## K-最近邻算法 KNN

#### 距离量度的选择

- 算术差（各分量的差的总和）
  - 坐标轴会影响结果的值
  - 更便于细致地研究各分量的意义
- 欧式差（空间上的距离）
  - 坐标轴不会影响结果的值
  - 较不考虑各分量的意义
        
#### 缺点   
1. **超参数** 较多。例如K的选取、上面的考量等。超参数是无法从数据中学习到的。
2. KNN算法使用的是数据点之间的**某种距离**，这不适合用于反应图像之间视觉的相似度
3. **维度灾难**。KNN算法意味着将空间分割，标签为不同事物。若我们想要好的效果，就需要data在空间中足够密集。而这将使数据集随维度增长而指数级上升。
        
#### 一些思维误区
1、在训练时使用不同的超参数K，最后选择效果最好者。问题出在你的
    这个超参数可能只是在这组训练集上好使。
    (QS：K不能当成普通参数来训练吗
        AS:不能，从k的意义即可明白。我们可以手动使用一组不同的k value来比对找出相对最好的k)
    解决这个问题更常见的方法是将dataset分为train、validation和test。严格分开最后两者。其中test一定是最后才用的（一般用了就不再修改算法，保证学术真实。

### 交叉验证 cross-Validation
    Meaning: 将训练集分为5组。循环5次，每次取其一为validation组，
        用同一组超参数训练，并在validation上验证。取其平均效果为结果。
    适用于小型训练，instead of deep learning。
    最后画一张坐标图，K的取值-在validationData的准确度。


*22/10/18*
## 合页损失函数 Hinge loss function

$$L_{i} = \frac{1}{N}\sum_{j \neq y_i}^{N}max(0, s_j - s_{y_i} + 1)$$


其中，i是当前testSample的序号，j是遍历label的索引，y是正确的labelSet，$s_{y_i}$是当前的testSample在correctLabel上获得的score

功能：对已有的W（在网络中训练出来的权值）的分类效果进行量化。

### Some Tips
（假设我们设置了为1的边界值，label总数为C）
1. 刚开始训练时，我们使用随机化的W跑loss()，通常出来效果是很平庸且平均的，即每个sample对各个label的score都接近0，这将使得每个sample的lossValue为 **C-1**。如果你跑出来不是这个样子，很可能你的code出了一些bug。
2. 我们不关心lossValue的具体数值，只关心lossValue之间的相对大小，因此我们求lossValue时再加上 ***mean*** 也无关紧要。
3. 我们还可以在求lossValue时在差值的基础上平方，这将使得大的误差更加明显。反之亦然。
   
### 优化: 正则化Regularization
虽然目前的loss()可以量化给定的W的classify effect，但实际上对于我们的testSet，能得到同样effect的W可能有多个。根据Occam's Razor(奥卡姆剃刀)原理，我们应该选择最简单的W。这时我们可以给loss()加上一个正则化参数: "$\lambda R(W)$"
$$L_{i} = \frac{1}{N}\sum_{j \neq y_i}^{N}max(0, s_j - s_{y_i} + 1) + \lambda R(W)$$

<font size = 2>其中$\lambda$是超参数，$R(W)$需要我们手动设置</font>


### 选择$R(W)$
$$欧式差L2：   R(W) = \sum_k\sum_l W_{k,l}^2 $$

$$算术差L1：   R(W) = \sum_k\sum_l|W_{k,l}| $$

$$综合（L1，L2）：   R(W) = \sum_k\sum_l \beta W_{k,l}^2 + |W_{k,l}|$$

L2的本质是，使得我们在跑loss()时，W在某些方向做更大努力才能取得更好的结果。概括地说，它是对某些w（权重向量）的欧式范数norm（空间距离的大小）的一个惩罚。

同理，L1的本质则是惩罚L1（算术差的值）。它还有一个作用就是鼓励稀疏（在讲什么呀